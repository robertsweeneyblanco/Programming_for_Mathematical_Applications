{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The `Optim` package\n",
    "\n",
    "For more advanced optimization method, Julia provided a number of state-of-the-art packages.\n",
    "Here we will consider the `Optim` package. It has a number of options and features, but\n",
    "here we will only demonstrate some basic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the simplest form, you can solve our optimization problem by only providing\n",
    "the function $f(x)$ and the initial guess $x_0$. The output shows that it found\n",
    "a local minimum, and that it used the \"Nelder-Mead\" method which is derivative\n",
    "free (zeroth order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     -2.072854e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    31\n",
       "    f(x) calls:    60\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = -sin(x[1]^2/2 - x[2]^2/4 + 3) * cos(2x[1] + 1 - exp(x[2])) # Same as last section\n",
    "res = optimize(f, [0, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If a gradient function is available, a gradient-based solver will be used\n",
    "automatically (in this case, the L-BFGS method). The `inplace=false` is used\n",
    "since our gradient function `df` returns the gradient instead of modifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     -2.072854e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     L-BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 9.09e-07 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.07e-06 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 1.19e-12 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 5.74e-12 ≰ 0.0e+00\n",
       "    |g(x)|                 = 5.19e-10 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    8\n",
       "    f(x) calls:    20\n",
       "    ∇f(x) calls:   20\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function df(x) \n",
    "    # Same as last section\n",
    "    a1 = x[1]^2/2 - x[2]^2/4 + 3\n",
    "    a2 = 2x[1] + 1 - exp(x[2])\n",
    "    b1 = cos(a1)*cos(a2)\n",
    "    b2 = sin(a1)*sin(a2)\n",
    "    return -[x[1]*b1 - 2b2, -x[2]/2*b1 + exp(x[2])*b2]\n",
    "end\n",
    "res = optimize(f, df, [0, 0.5]; inplace=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method can also be specified explicitly, e.g. the gradient descent method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     -2.072854e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Gradient Descent\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 2.20e-08 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 2.60e-08 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 1.75e-15 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 8.44e-15 ≰ 0.0e+00\n",
       "    |g(x)|                 = 5.64e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    24\n",
       "    f(x) calls:    65\n",
       "    ∇f(x) calls:   65\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = optimize(f, df, [0, 0.5], GradientDescent(); inplace=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the gradient function is not available, it can be computed using\n",
    "automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     -2.072854e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Gradient Descent\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 2.20e-08 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 2.60e-08 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 1.97e-15 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 9.51e-15 ≰ 0.0e+00\n",
       "    |g(x)|                 = 5.64e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    24\n",
       "    f(x) calls:    65\n",
       "    ∇f(x) calls:   65\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = optimize(f, [0, 0.5], GradientDescent(); autodiff=:forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This also works for the Hessian matrix in Newton's method. Note the extremely fast\n",
    "convergence (number of iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     -2.072854e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Newton's Method\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 4.82e-07 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 5.69e-07 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 1.69e-13 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 8.17e-13 ≰ 0.0e+00\n",
       "    |g(x)|                 = 2.90e-14 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    4\n",
       "    f(x) calls:    9\n",
       "    ∇f(x) calls:   9\n",
       "    ∇²f(x) calls:  4\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = optimize(f, [0, 0.5], Newton(); autodiff=:forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we use the BFGS solver using automatic differentiation. This is a widely\n",
    "used method, since it obtains convergence comparable to Newton's method but without\n",
    "requiring explicit Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     -2.072854e-01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.09e-05 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.28e-05 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 2.91e-10 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 1.41e-09 ≰ 0.0e+00\n",
       "    |g(x)|                 = 4.04e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    6\n",
       "    f(x) calls:    14\n",
       "    ∇f(x) calls:   14\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = optimize(f, [0, 0.5], BFGS(); autodiff=:forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}